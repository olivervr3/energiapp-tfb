\chapter{Análisis Big Data: Dataset UK-DALE y Metodología de Preprocesamiento}
\label{ch:big_data_analysis}

\section{Caracterización del Dataset UK-DALE}

\subsection{Descripción técnica del conjunto de datos}

El UK Domestic Appliance-Level Electricity (UK-DALE) dataset constituye el foundation dataset utilizado en este proyecto, representando uno de los conjuntos de datos más completos y técnicamente rigurosos disponibles para la investigación en disaggregation de consumo energético doméstico \cite{kelly2015uk}. 

\subsubsection{Especificaciones técnicas del dataset}

El UK-DALE dataset presenta las siguientes características técnicas fundamentales:

\textbf{Volumen de datos:} 4.4 años de datos continuos de consumo eléctrico de 5 viviendas del Reino Unido, totalizando 16.8 TB de información cruda antes del preprocesamiento.

\textbf{Resolución temporal:} Datos de agregado total registrados cada 6 segundos, con mediciones a nivel de dispositivo individual capturadas cada 1-8 segundos dependiendo del tipo de electrodoméstico.

\textbf{Cobertura de dispositivos:} 109 canales de medición individuales distribuidos entre:
\begin{itemize}
    \item 54 electrodomésticos mayores (frigoríficos, lavadoras, lavavajillas, hornos)
    \item 32 sistemas de iluminación (LED, halógenas, fluorescentes)
    \item 23 dispositivos electrónicos (televisores, ordenadores, equipos de audio)
\end{itemize}

\textbf{Metadata contextual:} Información detallada sobre características de las viviendas, incluyendo:
\begin{itemize}
    \item Superficie útil (87-219 m²)
    \item Número de ocupantes (2-4 personas)
    \item Año de construcción (1930-2006)
    \item Sistema de calefacción (gas natural, electricidad, bomba de calor)
    \item Clasificación energética (C-F según EPC)
\end{itemize}

\subsection{Análisis estadístico descriptivo del dataset}

\subsubsection{Distribución de consumo agregado}

El análisis estadístico descriptivo del consumo agregado revela patrones complejos que justifican la aplicación de técnicas avanzadas de machine learning:

\textbf{Estadísticas de tendencia central:}
\begin{itemize}
    \item Media: 762.4 W (σ = 441.3 W)
    \item Mediana: 645.2 W
    \item Coeficiente de asimetría: 2.17 (distribución fuertemente sesgada a la derecha)
    \item Curtosis: 7.89 (presencia de outliers significativos)
\end{itemize}

\textbf{Análisis de variabilidad temporal:}
La descomposición temporal mediante STL (Seasonal and Trend decomposition using Loess) revela:
\begin{itemize}
    \item Componente de tendencia: -2.3\% anual (mejora de eficiencia)
    \item Estacionalidad anual: amplitud de 186 W entre máximo (invierno) y mínimo (verano)
    \item Estacionalidad semanal: variación de 94 W entre días laborables y fines de semana
    \item Estacionalidad diaria: picos de 1,240 W (07:30-09:00) y 1,680 W (18:30-21:30)
\end{itemize}

\subsubsection{Caracterización por dispositivo individual}

El análisis granular por dispositivo revela heterogeneidad significativa en patrones de uso:

\textbf{Electrodomésticos de alto consumo (>1000W):}
\begin{table}[H]
\centering
\caption{Caracterización estadística de electrodomésticos de alto consumo}
\begin{tabular}{lrrr}
\toprule
\textbf{Dispositivo} & \textbf{Potencia Media (W)} & \textbf{Horas/día} & \textbf{Contribución (\%)} \\
\midrule
Lavadora & 1,847 & 1.2 & 12.3 \\
Lavavajillas & 1,623 & 0.8 & 7.1 \\
Calentador agua & 2,341 & 2.1 & 26.8 \\
Horno eléctrico & 2,089 & 0.6 & 6.9 \\
Secadora & 2,156 & 0.9 & 10.6 \\
\bottomrule
\end{tabular}
\label{tab:alto_consumo}
\end{table}

\textbf{Electrodomésticos de consumo continuo (<500W):}
\begin{table}[H]
\centering
\caption{Caracterización estadística de electrodomésticos de consumo continuo}
\begin{tabular}{lrrr}
\toprule
\textbf{Dispositivo} & \textbf{Potencia Media (W)} & \textbf{Factor carga} & \textbf{Contribución (\%)} \\
\midrule
Frigorífico & 142 & 0.35 & 8.9 \\
Standby TV & 23 & 0.78 & 4.2 \\
Router WiFi & 8 & 1.00 & 1.7 \\
Iluminación LED & 67 & 0.42 & 6.3 \\
Ordenador portátil & 45 & 0.61 & 6.1 \\
\bottomrule
\end{tabular}
\label{tab:consumo_continuo}
\end{table}

\section{Metodología de Preprocesamiento Big Data}

\subsection{Pipeline de procesamiento de datos}

La transformación del dataset crudo UK-DALE en un conjunto de datos apto para machine learning requiere un pipeline de preprocesamiento sofisticado que aborde múltiples desafíos técnicos inherentes a datos energéticos a gran escala.

\subsubsection{Fase 1: Limpieza y validación de datos}

\textbf{Detección de anomalías temporales:}
Implementación de algoritmos de detección de outliers multivariante para identificar mediciones anómalas:

\subsection{Detección y corrección de anomalías}

La detección de anomalías en datasets energéticos es crucial para garantizar la calidad de los datos utilizados en el entrenamiento de modelos predictivos. Las anomalías pueden surgir de fallos de sensores, errores de transmisión, o eventos excepcionales no representativos del comportamiento normal.

\begin{lstlisting}[language=Python, caption=Sistema de detección de anomalías]
from sklearn.ensemble import IsolationForest

def detect_energy_anomalies(data, contamination=0.01):
    # Crear features temporales específicas
    features = create_temporal_features(data)
    
    # Isolation Forest optimizado para datos energéticos
    iso_forest = IsolationForest(
        contamination=contamination,
        n_estimators=200,
        random_state=42
    )
    
    anomaly_labels = iso_forest.fit_predict(features)
    anomaly_scores = iso_forest.decision_function(features)
    
    # Retornar máscara de valores válidos
    valid_mask = anomaly_labels == 1
    return valid_mask, anomaly_scores

def create_temporal_features(data):
    # Features temporales: hour, day_of_week, month, is_weekend
    # Rolling statistics: power_ma_1h, power_std_24h
    # Lag features: power_lag_1, power_diff_24h
    # Seasonal decomposition features
    return enhanced_features
\end{lstlisting}

**Estrategia multi-nivel:** El sistema implementa detección de anomalías en múltiples niveles temporales (minutos, horas, días) para capturar diferentes tipos de irregularidades. Las anomalías a nivel de minutos pueden indicar picos de consumo genuinos, mientras que anomalías diarias sugieren comportamientos atípicos del usuario.

**Corrección adaptativa:** Una vez detectadas, las anomalías se corrigen utilizando interpolación inteligente que considera patrones estacionales y tendencias a largo plazo, preservando la estructura temporal inherente de los datos energéticos.

\subsection{Corrección de deriva temporal y sincronización multi-canal}

**Corrección de deriva temporal:** Los sensores energéticos experimentan deriva debido a factores ambientales. El sistema implementa corrección automática utilizando períodos de referencia conocidos (standby nocturno) para calibrar la deriva y mantener precisión a largo plazo.

\begin{lstlisting}[language=Python, caption=Pipeline de sincronización temporal]
def synchronize_multi_channel_data(channels_data, target_frequency='6S'):
    synchronized_channels = {}
    
    for channel_id, channel_data in channels_data.items():
        # Detectar y corregir timestamps duplicados
        clean_data = channel_data.loc[~channel_data.index.duplicated()]
        
        # Estrategias de resampleo según tipo de dispositivo
        if channel_data.attrs.get('type') == 'continuous':
            resampled = clean_data.resample(target_frequency).mean()
            resampled = resampled.interpolate(method='linear')
        elif channel_data.attrs.get('type') == 'discrete':
            resampled = clean_data.resample(target_frequency).max()
            resampled = resampled.fillna(0)
        
        synchronized_channels[channel_id] = resampled
    
    # Combinar y validar alineación temporal
    combined_df = pd.DataFrame(synchronized_channels)
    sync_quality = calculate_synchronization_quality(combined_df)
    
    return combined_df, sync_quality
\end{lstlisting}

**Sincronización multi-canal:** El UK-DALE dataset presenta desafíos de sincronización debido a diferentes frecuencias de muestreo. El algoritmo implementa estrategias específicas por tipo de dispositivo: interpolación lineal para electrodomésticos continuos (frigorífico), y agregación máxima para dispositivos discretos (lavadora).

**Métricas de calidad:** El sistema calcula métricas de calidad de sincronización incluyendo porcentaje de datos faltantes, duración máxima de gaps, y scores de alineación temporal basados en correlación cruzada entre canales.

\subsection{Feature Engineering avanzado para datos energéticos}

El proceso de feature engineering implementa múltiples categorías de características especializadas para capturar patrones complejos en los datos de consumo energético:

\textbf{Features cíclicas temporales:} Se implementan transformaciones trigonométricas para capturar periodicidad en múltiples escalas temporales (hora del día, día del año, día de la semana), preservando la continuidad en los límites de los ciclos.

\textbf{Features de calendario extendidas:} Incluyen identificación de días laborables, detección de festivos del Reino Unido, y clasificación estacional, proporcionando contexto social y cultural al consumo energético.

\textbf{Features de lag temporal:} Se calculan características de retardo en múltiples horizontes temporales (6 segundos, 1 minuto, 24 horas, 7 días) para capturar dependencias temporales de corto y largo plazo.

\textbf{Features de ventana deslizante:} Implementan estadísticas descriptivas (media, desviación estándar, mínimo, máximo, rango, asimetría, curtosis) en ventanas temporales múltiples para caracterizar la variabilidad del consumo.

\textbf{Features de frecuencia (FFT):} Mediante análisis de Fourier, se extraen características espectrales incluyendo frecuencia dominante, distribución de energía en bandas de frecuencia, y centroide espectral para identificar patrones periódicos complejos.

\subsection{Optimización de almacenamiento y acceso a datos}

\subsubsection{Arquitectura de datos distribuida}

Para manejar eficientemente los 16.8 TB del dataset UK-DALE, implementamos una arquitectura de almacenamiento optimizada:

\textbf{Particionamiento temporal inteligente:}

\begin{lstlisting}[language=Python, caption=Sistema de particionamiento temporal]
import pyarrow as pa
import pyarrow.parquet as pq
from pathlib import Path

class EnergyDataPartitioner:
    """
    Sistema de particionamiento optimizado para datos energéticos temporales
    """
    
    def __init__(self, base_path, partition_strategy='monthly'):
        self.base_path = Path(base_path)
        self.partition_strategy = partition_strategy
        
    def partition_dataset(self, data, metadata):
        """
        Particiona el dataset usando estrategia temporal optimizada
        """
        if self.partition_strategy == 'monthly':
            return self._partition_monthly(data, metadata)
        elif self.partition_strategy == 'weekly':
            return self._partition_weekly(data, metadata)
        else:
            raise ValueError(f"Unknown partition strategy: {self.partition_strategy}")
    
    def _partition_monthly(self, data, metadata):
        """
        Particionamiento mensual con compresión optimizada
        """
        # Crear esquema Parquet optimizado
        schema = pa.schema([
            pa.field('timestamp', pa.timestamp('us')),
            pa.field('power', pa.float32()),
            pa.field('device_id', pa.string()),
            pa.field('house_id', pa.int8()),
            pa.field('year', pa.int16()),
            pa.field('month', pa.int8()),
            pa.field('day', pa.int8()),
            pa.field('hour', pa.int8()),
            pa.field('minute', pa.int8())
        ])
        
        # Particionar por año/mes
        for (year, month), group in data.groupby([data.index.year, data.index.month]):
            partition_path = self.base_path / f"year={year}" / f"month={month:02d}"
            partition_path.mkdir(parents=True, exist_ok=True)
            
            # Convertir a PyArrow Table con schema optimizado
            table = pa.Table.from_pandas(
                group.reset_index(), 
                schema=schema,
                preserve_index=False
            )
            
            # Escribir con compresión y configuración optimizada
            pq.write_table(
                table,
                partition_path / "data.parquet",
                compression='snappy',
                use_dictionary=True,
                row_group_size=100000,
                use_deprecated_int96_timestamps=False
            )
            
            # Escribir metadata de partición
            partition_metadata = {
                'records_count': len(group),
                'start_date': group.index.min().isoformat(),
                'end_date': group.index.max().isoformat(),
                'devices': group['device_id'].unique().tolist(),
                'avg_power': float(group['power'].mean()),
                'total_energy_kwh': float(group['power'].sum() / (1000 * 6 * 60))  # 6s intervals
            }
            
            with open(partition_path / "metadata.json", 'w') as f:
                json.dump(partition_metadata, f, indent=2)

class OptimizedDataLoader:
    """
    Cargador de datos optimizado para consultas eficientes
    """
    
    def __init__(self, data_path):
        self.data_path = Path(data_path)
        self.partition_index = self._build_partition_index()
    
    def _build_partition_index(self):
        """
        Construye índice de particiones para consultas rápidas
        """
        index = {}
        for partition_dir in self.data_path.rglob("metadata.json"):
            with open(partition_dir) as f:
                metadata = json.load(f)
            
            partition_key = partition_dir.parent.name
            index[partition_key] = {
                'path': partition_dir.parent / "data.parquet",
                'date_range': (metadata['start_date'], metadata['end_date']),
                'devices': metadata['devices'],
                'records': metadata['records_count']
            }
        
        return index
    
    def load_date_range(self, start_date, end_date, devices=None):
        """
        Carga datos para un rango de fechas específico con filtrado optimizado
        """
        relevant_partitions = self._find_relevant_partitions(start_date, end_date)
        
        dataframes = []
        for partition_info in relevant_partitions:
            # Usar filtros de PyArrow para lectura eficiente
            filters = [
                ('timestamp', '>=', start_date),
                ('timestamp', '<=', end_date)
            ]
            
            if devices:
                filters.append(('device_id', 'in', devices))
            
            df = pq.read_table(
                partition_info['path'],
                filters=filters,
                columns=['timestamp', 'power', 'device_id']  # Solo columnas necesarias
            ).to_pandas()
            
            dataframes.append(df)
        
        if dataframes:
            combined_df = pd.concat(dataframes, ignore_index=True)
            return combined_df.set_index('timestamp').sort_index()
        else:
            return pd.DataFrame()
    
    def _find_relevant_partitions(self, start_date, end_date):
        """
        Encuentra particiones relevantes para el rango de fechas
        """
        relevant = []
        for partition_key, info in self.partition_index.items():
            part_start = pd.to_datetime(info['date_range'][0])
            part_end = pd.to_datetime(info['date_range'][1])
            
            # Verificar solapamiento de rangos
            if not (end_date < part_start or start_date > part_end):
                relevant.append(info)
        
        return relevant
\end{lstlisting}

\section{Validación y métricas de calidad de datos}

\subsection{Framework de validación integral}

La validación de la calidad de datos constituye un aspecto crítico que determina la confiabilidad de los resultados del análisis. Implementamos un framework de validación multi-nivel:

\begin{lstlisting}[language=Python, caption=Framework de validación de calidad de datos]
class DataQualityValidator:
    """
    Framework integral de validación de calidad para datos energéticos
    """
    
    def __init__(self):
        self.validation_rules = self._define_validation_rules()
        self.quality_metrics = {}
    
    def _define_validation_rules(self):
        """
        Define reglas de validación específicas para datos energéticos
        """
        return {
            'power_range': {
                'min_value': 0,
                'max_value': 10000,  # 10kW máximo razonable para hogar
                'description': 'Potencia dentro de rangos físicamente posibles'
            },
            'power_rate_change': {
                'max_change_per_second': 5000,  # 5kW/s máximo cambio
                'description': 'Tasa de cambio de potencia físicamente posible'
            },
            'temporal_consistency': {
                'max_gap_minutes': 10,
                'expected_frequency': '6S',
                'description': 'Consistencia temporal de mediciones'
            },
            'device_consistency': {
                'min_standby_power': 0,
                'max_continuous_power_hours': 24,
                'description': 'Consistencia específica por tipo de dispositivo'
            }
        }
    
    def validate_dataset(self, data, device_metadata):
        """
        Ejecuta validación completa del dataset
        """
        validation_results = {}
        
        # Validación de rangos de potencia
        validation_results['power_range'] = self._validate_power_range(data)
        
        # Validación de consistencia temporal
        validation_results['temporal'] = self._validate_temporal_consistency(data)
        
        # Validación de tasa de cambio
        validation_results['rate_change'] = self._validate_rate_change(data)
        
        # Validación específica por dispositivo
        validation_results['device_specific'] = self._validate_device_specific(data, device_metadata)
        
        # Validación de correlaciones físicas
        validation_results['physical_correlations'] = self._validate_physical_correlations(data)
        
        # Calcular score global de calidad
        overall_quality_score = self._calculate_overall_quality_score(validation_results)
        
        return {
            'validation_results': validation_results,
            'quality_score': overall_quality_score,
            'recommendations': self._generate_quality_recommendations(validation_results)
        }
    
    def _validate_power_range(self, data):
        """
        Valida que los valores de potencia estén en rangos razonables
        """
        rule = self.validation_rules['power_range']
        
        invalid_values = (
            (data['power'] < rule['min_value']) | 
            (data['power'] > rule['max_value'])
        )
        
        return {
            'invalid_count': invalid_values.sum(),
            'invalid_percentage': (invalid_values.sum() / len(data)) * 100,
            'invalid_indices': data.index[invalid_values].tolist()[:100],  # Primeros 100
            'rule_applied': rule,
            'passed': invalid_values.sum() == 0
        }
    
    def _validate_temporal_consistency(self, data):
        """
        Valida consistencia temporal de las mediciones
        """
        rule = self.validation_rules['temporal_consistency']
        
        # Calcular gaps temporales
        time_diffs = data.index.to_series().diff().dt.total_seconds()
        expected_interval = pd.Timedelta(rule['expected_frequency']).total_seconds()
        
        # Detectar gaps significativos
        large_gaps = time_diffs > (expected_interval * 10)  # >10x intervalo esperado
        
        # Detectar duplicados temporales
        duplicate_timestamps = data.index.duplicated()
        
        return {
            'large_gaps_count': large_gaps.sum(),
            'duplicate_timestamps': duplicate_timestamps.sum(),
            'median_interval_seconds': time_diffs.median(),
            'expected_interval_seconds': expected_interval,
            'irregular_intervals_percentage': ((time_diffs - expected_interval).abs() > 1).mean() * 100,
            'passed': large_gaps.sum() == 0 and duplicate_timestamps.sum() == 0
        }
    
    def _validate_rate_change(self, data):
        """
        Valida tasas de cambio físicamente posibles
        """
        rule = self.validation_rules['power_rate_change']
        
        # Calcular tasa de cambio por segundo
        power_diff = data['power'].diff()
        time_diff = data.index.to_series().diff().dt.total_seconds()
        rate_change = power_diff / time_diff
        
        # Detectar cambios excesivos
        excessive_changes = rate_change.abs() > rule['max_change_per_second']
        
        return {
            'excessive_changes_count': excessive_changes.sum(),
            'max_observed_rate': rate_change.abs().max(),
            'max_allowed_rate': rule['max_change_per_second'],
            'percentile_95_rate': rate_change.abs().quantile(0.95),
            'passed': excessive_changes.sum() < len(data) * 0.001  # <0.1\% permitido
        }
    
    def _validate_physical_correlations(self, data):
        """
        Valida correlaciones físicamente esperadas entre variables
        """
        correlations = {}
        
        if 'total_power' in data.columns and len([col for col in data.columns if 'device_' in col]) > 1:
            # Validar que suma de dispositivos aproximadamente igual a total (considerando pérdidas)
            device_columns = [col for col in data.columns if 'device_' in col]
            device_sum = data[device_columns].sum(axis=1)
            total_power = data['total_power']
            
            # Calcular diferencia relativa
            relative_diff = ((total_power - device_sum) / total_power).abs()
            
            correlations['total_vs_sum'] = {
                'correlation': total_power.corr(device_sum),
                'mean_relative_diff': relative_diff.mean(),
                'max_relative_diff': relative_diff.max(),
                'passed': relative_diff.mean() < 0.15  # <15\% diferencia promedio
            }
        
        return correlations
    
    def _calculate_overall_quality_score(self, validation_results):
        """
        Calcula score global de calidad (0-100)
        """
        weights = {
            'power_range': 0.3,
            'temporal': 0.25,
            'rate_change': 0.2,
            'device_specific': 0.15,
            'physical_correlations': 0.1
        }
        
        score = 0
        for category, weight in weights.items():
            if category in validation_results:
                category_score = self._calculate_category_score(validation_results[category])
                score += weight * category_score
        
        return min(100, max(0, score))
    
    def _calculate_category_score(self, category_results):
        """
        Calcula score para una categoría específica
        """
        if isinstance(category_results, dict) and 'passed' in category_results:
            if category_results['passed']:
                return 100
            else:
                # Score basado en severidad de fallos
                if 'invalid_percentage' in category_results:
                    return max(0, 100 - category_results['invalid_percentage'] * 10)
                else:
                    return 50  # Score neutro si no hay info específica
        
        return 75  # Score por defecto
\end{lstlisting}

Esta metodología integral de Big Data asegura que el dataset UK-DALE sea procesado con los más altos estándares de calidad científica, proporcionando una base sólida para los algoritmos de machine learning subsecuentes.
